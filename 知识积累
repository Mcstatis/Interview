一、快速排序
def partition(alist, first, last):
    n = len(alist)
    pivot_value = alist[first]
    left = first + 1
    right = last
    while left <= right:
        if alist[left] > pivot_value and alist[right] < pivot_table:
            alist[left], alist[right] = alist[right], alist[left]
            left += 1
            right -= 1
        elif alist[left] < pivot_value:
            left += 1
        elif alist[right] > pivot_value:
            right -= 1
    alist[first], alist[right] = alist[right], alist[first]
    return right
  
def quick_sort_helper(alist, first, last):
    if first < last:
        split_point = partition(alist, first, last)
        quick_sort_helper(alist, first, split_point-1)
        quick_sort_helper(alist, split_point+1, right)

def quick_sort(alist):
    first = 0
    last = len(alist) - 1
    quick_sort_helper(alist, first, last)
    return alist

二、二分查找
1、递归版
def bisec_find(alist, item):
    if len(alist) == 0:
        return False
    else: 
        mid = len(alist) // 2
        if len(alist) > 1:
            if alist[mid] == item:
                return True
            elif alist[mid] > item:
                return bisec_find(alist[:mid], item)
            else:
                return bisec_find(alist[mid+1:], item)
        
2、迭代版
def bisec_find(alist, item):
    n = len(alist)
    left = 0
    right = n - 1
    while left < right:
        mid = (left + right) // 2
        if alist[mid] == item:
            return True
            break
        elif alist[mid] > item:
            right = mid - 1
        else:
            left = mid + 1
    return False


三、简要介绍一下SVM

四、SVM中常用的核函数
核函数将原来的输入空间映射到一个新的特征空间，在新的特征空间里从训练样本中学习线性支持向量机。
当特征函数是非线性时，学习到的含有核函数的支持向量机就是非线性分类模型
    1、线性核
    2、多项式核
    3、高斯核（径向基）
    

五、SVM和LR的异同


六、过拟合(overfitting)怎么解决？
其直观的表现如下：随着训练过程的进行，模型复杂度的增加，在训练集上的误差渐渐减小，但是在验证集上的误差反而增大
泛化(generalization) 训练集 测试集
原因：噪声数据
    1、增大样本量（如果可以）
        Data Augmentation 数据扩增，图像识别中的平移、旋转和缩放
    2、Regularization(正则化)
    3、Cross Validation(交叉验证)
    4、Dropout(随机失活)
    在神经网络的训练过程中，神经元以超参数p的概率被激活(即1-p的概率被屏蔽)，其效果类似于集成学习中Bagging
    5、Batch Normalization(逐层归一化)
    每一层的输出都做归一化
    6、Early Stopping(提前终止)

七、Logistic Regression的原理

八、GBDT和XGBoost的区别
XGBoost本质上还是GBDT，其优化主要体现在工程设计层面，包括并发的程序执行，贪心的排序操作等
    1、GBDT以CART为基分类器，XGBoost还支持线性分类器
    2、GBDT在优化时只用到一阶导数的信息（负梯度），而XGBoost则对损失函数进行二阶泰勒展开，同时用到一阶和二阶导数信息；且XGBoost支持自定义损失函数，
只要函数是一阶和二阶可导
    3、XGBoost在损失函数中加入了正则项，控制模型的复杂度，有效防止过拟合，提高模型的泛化能力
    正则项中包括（1）树的叶子节点个数（2）每个叶子节点上输出的score的L2模的平方和
    4、XGBoost支持并行
    XGBoost的并行不是tree粒度的并行，作为Boosting方法，同样是一次迭代完再进行下一次的迭代。XBGoost的并行体现在特征粒度上：
    （1）近似算法（Weighted Quantile Sketch）
    树的分裂过程最耗时的一个步骤就是对特征的值进行排序，采用贪心算法枚举所有可能的分割节点，确定最佳分割节点
    XGBosot在训练之前，预先对数据进行了排序，然后保存为block结构，后面的迭代重复使用这个结构，大大减少计算量，这个block结构也使得并行成为可能。
    近似算法（分桶后再greedy搜索）中很重要的一步就是提出备选分割点，通常情况下，均匀挑选一个特征的百分位数作为候选。
    对于每个分割点，其函数值是用此分位点下包含的二阶导之和的比例。
    即相邻的两个分割点之间的间隔小于给定的ε。这里的ε是一个近似系数。直观上看，这大致表示，总共选出1/ε个候选点。这里每一个点都按照其二阶导h进行加权。
    （2）近似直方图算法（Distributed Weighted Quantile Sketch）
    贪心算法在数据无法一次载入内存或者在分布式情况下变得低效，可并行的近似直方图算法，高效地生成候选分割点
    5、shrinkage（缩减）
    相当于学习速率。XGBoost在进行完一次迭代后，会将叶子节点的权重乘上该系数，主要是为了削弱每棵树的影响，为后面的树去提供空间优化模型
    6、column subsampling
    列(特征)抽样，随机森林那边学习来的，防止过拟合的效果比传统的行抽样还好（行抽样功能也有），并且有利于后面提到的并行化处理算法
    7、缺失值的处理
    当特征出现缺失值时，XGBoost可以学习出默认的节点分裂方向。
    Xgboost的处理方式是，缺失值数据会被分到左子树和右子树分别计算损失，选择较优的那个（Max Gain）。
    如果训练中没有数据缺失，预测时出现了数据缺失，默认分类到右子树。
    
    https://blog.csdn.net/v_JULY_v/article/details/81410574
    通俗理解kaggle比赛大杀器xgboost
    July

九、为什么XGBoost要使用二阶泰勒展开？优势在哪里？
    1、有利于梯度下降得更快更准
    2、可以在不选定损失函数具体形式的情况下，仅仅依靠输入数据的值就可以进行叶子分裂计算。本质上是把损失函数的选取和参数优化分开。这种去耦合增加了XGBoost
    的适用性，使得它可以按需选择损失函数。
    
十、LR和线性回归的区别和联系
    1、目的
        分类 回归
    2、损失函数
        对数似然损失函数 平方和损失函数

十一、生成式模型和判别式模型
    生成式：由数据直接学习决策函数Y=f(x)或者条件分布概率P(Y|x)作为预测模型
    判别式：先由数据学习联合概率密度P(x,y)，再求出条件概率密度P(Y|x)
    
十二、CNN最成功的应用是在CV，那为什么NLP和Speech的很多问题也可以用？这几个不相关问题的相似性在哪里？CNN通过什么手段抓住了这些共性？
    都存在整体和局部的关系 Global Local
    由低层次的特征组合成高层次的特征 Low-level High-level
    CNN抓住共性的手段主要有4个：
    1、局部连接（也称稀疏交互Sparse Interaction）
    提取数据的局部特征
    2、权值共享（也称参数共享Parameter Sharing）
    权值共享大大降低了网络的训练难度，一个卷积核（Filter）只提取一个特征
    3、池化操作
    池化操作与多层次结构一起，实现了数据的降维，将低层次的局部特征组合成较高层次的特征，从而实现对整体图片的表示
    4、多层次结构

十三、LSTM结构推导？为什么比RNN好？LSTM如何解决梯度消失/梯度爆炸的问题？
    CNN → RNN(梯度消失/梯度爆炸) → LSTM
    RNN反向传播更新参数采用的链式法则，会出现梯度消失/梯度爆炸的问题
    
十四、拼写检查/拼写纠正
    基于贝叶斯方法

十五、对比pLSA和LDA的区别

十六、为什么要归一化？
    1、提升梯度下降法求解最优解的速度（少走弯路）
    2、距离计算中，消除量纲的影响（KNN / SVM）
    一般算法如果本身受量纲影响较大（KNN / SVM），或者损失函数（最优化方法采用梯度下降）受量纲影响较大（线性回归 / 逻辑回归），则需要对特征进行归一化。
    LR本身不收量纲的影响，但是采用梯度下降法优化参数时如果不进行归一化会导致参数迭代缓慢，影响效率。
    
十七、LR为什么要对输入的连续特征进行离散化？
    1、离散化后的特征对异常值具有很好的鲁棒性
    2、单个连续性变量离散化为N个分类型变量后，每个变量都有其单独的权重，相当于为为模型引入了非线性性，提升模型的表达能力，减少欠拟合
    3、离散化后可以进行特征交叉，由M+N个变量变成M×N个变量，进一步引入非线性性，提升模型表达能力
    李沐曾经说过：模型是使用离散特征还是连续特征，其实是一个“海量离散特征+简单模型” 同 “少量连续特征+复杂模型”的权衡。
    既可以离散化用线性模型，也可以用连续特征加深度学习。就看是喜欢折腾特征还是折腾模型了。
    通常来说，前者容易，而且可以n个人一起并行做，有成功经验；后者目前看很赞，能走多远还须拭目以待。

https://blog.csdn.net/jifeihuiaoguai988/article/details/51378108  EM算法介绍

十八、L1和L2正则的先验分别服从什么分布？
    L1 拉普拉斯分布  在均值附近，比正态分布更为陡峭（位置参数mu，尺度参数sigma）
    L2 正态分布
    
十九、完整的机器学习项目的流程
    误差分析/模型诊断
    误差分析：通过观察误差样本，全面分析误差产生的原因，是特征的问题还是数据本身的问题？是算法选择的问题还是参数的问题？
    诊断后的模型需要重新进行调优，调优后的模型需要重新进行诊断，不断循环往复，达到预期效果
    
    上线运行
    运行速度（时间复杂度）
    资源消耗（空间复杂度）
    稳定性
    
二十、梯度消失和梯度爆炸
    1、梯度消失（Gradient Vanish）
    2种情况，梯度消失经常出现
        1）深层网络中
        基于反向传播进行权值更新的过程中，根据链式法则进行连乘，如果激活函数求导值<1，随着层数的增加，求出的权重更新将以指数级衰减，即发生梯度消失
        2）采用不合适的损失函数
        比如Sigmoid，tanh比sigmoid要好一些，但是它的导数仍然是小于1
    2、 梯度爆炸（Gradient Explosion）
        1）深层网络中
        2）权值初始化值太大
    3、如何解决？
        1）梯度剪切
        设置一个梯度剪切阈值，将其强行限定在某个区间范围
        2）权重正则化
        避免过大的权值出现，防止梯度爆炸
        3）激活函数
        ReLU        
            优：解决了梯度消失、爆炸的问题；计算方便，速度快，加速了网络的训练速度
            缺：负数部分恒为0，导致神经元无法激活
        4）Batch Normalization（批量归一化）
        加速网络训练速度，提升训练稳定性
        5）残差结构
        残差的跨层连接结构，即残差的捷径（shortcut）部分
        6）LSTM
        门控机制
https://blog.csdn.net/qq_25737169/article/details/78847691
https://blog.csdn.net/qq_25737169/article/details/79048516

    4、如何解决神经网络中的梯度爆炸问题？
        1、重新设计网络结构（优化）
        层数更少/批尺寸更小
        RNN → LSTM
        2、选取新的激活函数
        sigmoid/tanh → Relu
        3、梯度截断（Gradient Clipping）
        4、权重正则化（Weight Regularization）

二十一、什么是卷积？
    对图像的局部数据窗口和滤波矩阵做内积（对应位置元素相乘再相加）的操作即为卷积。

二十二、什么是池化？
    取区域内的平均或者最大
    局部连接
    
二十三、简述下什么是生成对抗网络？
    生成对抗网络（Generative Adversarial Network），其内部是竞争关系。有点类似“以子之矛，攻子之盾”在相爱相杀中共同成长。
    一方是生成模型（Generative Model），另一方是判别模型（Discriminative Model）
    生成模型的任务是生成一个实例来骗过判别模型，而判别模型的任务则是判断这个实例是来自训练样本（真实）还是生成模型（合成）。
    双方相互对抗，最终达到一个平衡。

二十四、HMM和CRF的区别
    1、模式
    HMM：生成式
    CRF：判别式
    2、是否先验
    HMM：加入对状态概率的先验知识
    CRF：完全是Data Driven

二十五、k-d树
    k-d树（k-dimension tree的缩写）一种空间划分树，其本质是一种平衡二叉树，主要应用于多维空间关键数据的搜索（空间搜索和最近领搜索）
    k-d树的构建是一个递归的过程。采用分而治之的思想，将整个空间划分几个小的空间。
    确定Split域：对于所有描述子数据（特征矢量），统计他在每个维度的方差，选取方差中的最大值，对应的就是split域。方差最大表示沿着这个
    坐标轴方向数据分散的比较开，这个方向进行分割能获得比较好的分辨率。
    k-d树构建后，如何进行最近领搜索 https://img-my.csdn.net/uploads/201206/03/1338711447_6884.gif
    
    https://blog.csdn.net/v_july_v/article/details/8203674
    
二十六、LSTM如何解决梯度消失的问题？
    https://www.cnblogs.com/xumaomao/articles/11071744.html
    RNN 中的梯度消失/梯度爆炸和普通的 MLP 或者深层 CNN 中梯度消失/梯度爆炸的含义不一样。
    MLP/CNN 中不同的层有不同的参数，各是各的梯度；而RNN中同样的权重在各个时间步共享，最终梯度等于各个时间步梯度的和
    RNN 中总的梯度是不会消失的。即便梯度越传越弱，那也只是远距离的梯度消失，由于近距离的梯度不会消失，所有梯度之和便不会消失。
    RNN 所谓梯度消失的真正含义是，梯度被近距离梯度主导，导致模型难以学到远距离的依赖关系。
    LSTM 仍然有可能发生梯度爆炸。不过，由于 LSTM 的其他路径非常崎岖，和普通 RNN 相比多经过了很多次激活函数（导数都小于 1），
    因此 LSTM 发生梯度爆炸的频率要低得多。实践中梯度爆炸一般通过梯度裁剪来解决。
    f遗忘门介于[0, 1]之间的情况，在这种情况下只能说 LSTM 改善（而非解决）了梯度消失的状况。
    
    https://www.cnblogs.com/bonelee/p/10475453.html

二十七、PCA的主要步骤？
    
二十八、LDA(Latent Dirichlet Allocation)和PLSA的区别
    在贝叶斯框架下的LDA，我们不再认为主题分布（文档中各个主题的出现概率）和词分布（主题中各个词语出现的概率分布）不是唯一确定的，而是随机变量
    有点类似频率学派和贝叶斯学派的争议

二十九、为什么需要对数据进行归一化？
    （1）有利于加速梯度下降法的收敛速度，对于使用梯度下降进行参数优化的算法
    （2）涉及距离计算、模型中含有公式，量纲的存在会干扰参数估计的结果，最终体现的是特征的重要性
    树形模型（概率模型）不需要归一化

三十、深度（监督）学习在计算机视觉（CV）领域的进展
    1、图像分类（Image Classification）
    ImageNet大规模图像识别竞赛
    各种CNN的变种：
       AlexNet(2013 83.6%) → VGG(2014 92.7%) → GoogleNet(2014 93.3%) → ResNet（2015 96.43% 微软 识别正确率超过人类水平） 
    2、图像/目标检测（Image/Object Detection）
    图像检测是指在分类图像的同时把物体用矩形框圈定起来 Classification + Localization
    先后涌现R-CNN(Region CNN) → Fast R-CNN() → Faster R-CNN() → YOLO → SSD
    https://blog.csdn.net/v_JULY_v/article/details/80170182  一文读懂目标检测：R-CNN、Fast R-CNN、Faster R-CNN、YOLO、SSD
    3、图像分割（Senmatic Segmentation）
    图像分割是把图片中不同的物体用不同的颜色分割出来
    4、图像标注-看图说话（Image Captioning）
    根据一张图片，生成一段文字描述它
    5、图像生成-文字转图片（Image Generator）
    根据一段文字，生成对应的图片

三十一、深度学习中的调参经验
    1、参数初始化
    不进行初始化会减慢收敛速度，影响收敛结果，甚至会造成Nan等一系列问题
    （1）uniform均匀分布初始化
    （2）normal高斯分布初始化
    （3）svd初始化 对RNN有比较好的效果
    2、数据预处理
    zero-center
    3、梯度消失/爆炸
    Gradient Clipping梯度裁剪，限制最大梯度
        附：梯度裁剪的执行步骤
            （1）设定超参数梯度阈值clip_grad的值
            （2）在back propagation的过程中得到各个参数的梯度后，并不直接用来更新参数。而是先计算这些梯度的L2范数L2_grad
            （3）比较L2_grad与clip_grad阈值的大小，如果L2_grad > clip_grad，计算缩放因子scale=clip_grad/L2_grad，否则，scale=1
            （4）g_origin×scale进行缩放，然后进行参数更新
    替代激活函数Sigmoid → tanh → ReLU
    梯度归一化
    4、防止过拟合
    Dropout
    Batch Normalization
    
三十二、RNN、LSTM和GRU的区别
    RNN：处理序列型数据，引入循环的概念，但是在实际过程中却出现初始信息随着时间消失的问题
    LSTM：门控机制，非常适合记忆，解决梯度消失的问题
    GRU：门控循环单元 Gated Recurrent Unit 更新门和重置门 LSTM的一种变体，将遗忘门和输入门合并成更新门

三十三、当机器学习遇到瓶颈时，如何优化？
    https://blog.csdn.net/han_xiaoyang/article/details/52654879 如何提高机器学习和深度学习的性能
    https://blog.csdn.net/han_xiaoyang/article/details/53453145 机器学习性能改善备忘单
    值得细细品读
    
三十四、什么样的资料不适合做深度学习？
    1、数据集太小时
    深度学习相对机器学习没有优势
    2、数据集没有局部相关性
    目前，在深度学习表现比较好的领域主要有图像、语音、文本（自然语言）领域，这些领域的一个共性是局部相关性：
        图像由像素组成物体、语音信号中音位组成单词、文本数据中单词组成句子，这些特征元素的组合一旦被打乱，表示的含义也会随之变化
        对于没有这样的局部相关性的数据集，不适于使用深度学习算法进行处理。

三十五、准备机器学习面试，应该准备哪些理论知识？
    https://www.zhihu.com/question/62482926  知乎问题：面试官如何判断面试者的机器学习水平？
    其中，2个问题回答比较经典，王喆和知乎用户
    
三十六、RNN的变体 N vs M
    这种结构又叫做Encoder-Decoder模型，也可以称之为Seq2seq模型
    原始的N vs N RNN要求序列等长，然而我们遇到的大部分问题都是序列不等长的。例如机器翻译中，源语言和目标语言的句子往往并没有相同的长度。
    由于这种Encoder-Decoder模型结构不限制输入和输出序列的长度，因此其应用范围十分广泛
    机器翻译 Encoder-Decoder的最经典应用
    文本摘要 输入一段文本序列，输出是这段文本序列的摘要
    阅读理解 输出文章和问题编码，再对其进行解码得到问题的答案
    语音识别 输入语音信号，输出文本序列
    图像标注 看图说话
    
    
三十八、深度学习（CNN/RNN/Attention）解决大规模文本分类问题
    1、传统文本分类方法
    人工特征工程 + 浅层分类模型
    （1）文本预处理：分词（研究表明，特征粒度为词远好于为字，基于字粒度显然损失了过多的n-gram信息） 
                    去停用词 维护一个停用词表
    （2）文本表示和特征处理
    传统方法：词袋（BOW Bag of Word）或者向量空间模型（Vector Space Model）
    缺陷：无法捕捉上下文关系，每个词之间彼此独立，且无法表征语义信息
        词袋法：高维稀疏
        向量空间模型：特征提取部分对应特征选择和特征权重计算（TF-IDF）两个部分
    （3）基于语义的文本表示
    LDA和pLSA文档主题模型
    
    2、现代分词
    Word Embedding + Bi-LSTM + CRF逐渐成为主流
    
    3、深度学习方法
    FastText
    TextCNN（捕捉局部相关性，具体到文本分类任务中就是利用CNN来提取句子中的n-gram关键信息）
    TextRNN
    TextRNN + Attention
    
三十九、对偶问题
    primal/dual 
    给出原始问题（主问题）最优解的下界
    对偶问题是凸优化问题，可以进行很好的求解

四十、推荐系统技术演进趋势
    经典的两阶段：多路召回(Retrieve/Match) + 统一排序(Rank)
    召回强调快：少量特征 + 简单模型
    排序强调准：丰富特征 + 复杂模型
    重排层：去已读、去重、打散、多样性保证、固定类型物品插入（往往是技术和业务策略，提升用户体验）
    1、召回层
    偏向策略型导向，往往想到一个策略，增加一路新的召回。
    召回环节的有监督模型化和一切Embedding化，这是两个相辅相成的发展趋势。
    根据召回路是否有个性化因素区分：
    （1）无个性化
    热门文章 点击率排序
    （2）个性化
    用户兴趣标签
    理论上，任何监督模型都可以用来做召回模型（FM/FFM/DNN）
    常说的所谓“双塔”，其实就是用户侧和物品侧特征分离，分别打上Embedding，并非具体的模型
    如果在召回阶段使用模型，那么应该同步采用和排序阶段相同的优化目标，尤其是排序阶段如果采用多目标优化，那么召回阶段也应当采用多目标优化
    
    知识图谱融合召回
    知识图谱是一种信息拓展模式，对知识进行近距离的拓展，可以起到信息补充作用
    比较适合用户行为非常稀疏以及用户冷启动的场景
    这种知识拓展，适用于召回阶段，因为对于召回来讲，精准并不是最重要的，找出和用户兴趣一定相关但又具有泛化性能是召回侧的重点。

    2、排序层
    （1）显式特征组合
    特征工程以及特征组合的自动化一直是推动推荐系统技术演进最主要的方向，没有之一
    LR → GBDT + LR → FM → FFM → DNN(Wide&Deep / DeepFM / xDeepFM)
    （2）特征抽取器演化
    低阶特征组合 FM function
    高阶特征组合 MLP
    CNN/RNN捕获局部特征关联方面是非常有效的结构，但是并不太适合单纯特征输入的推荐模型，因为在推荐领域，特征在输入次序上并没有必然的次序关系，
    基本属于人工随机定义。
    
    增强学习在推荐系统的应用
    利用增强学习来做推荐系统，有几个显而易见的好处：
    1、容易对“探索-利用Exploration-Exploitation”建模
    探索 往往意味着牺牲短期利益，毕竟探索的效率比较低
    2、比较容易体现用户兴趣的动态变化
    通过用户行为和反馈的物品对应的回报的重要性，而动态对推荐结果产生变化
    3、有利于长期收益实现
    我们优化推荐系统，往往会在意一些短期的目标：比如增加点击率
    
    多目标优化
    点击 互动（点赞/评论/转发）时长等多个目标的同时优化
    
    重排序技术演进
    Learning  to Rank
    推荐系统里Learning to Rank做排序，我们常见有3种优化目标：Point-wise Pair-wise List-wise
    首先明确一点：List-wise并不是指某个/某类具体的模型，而是指模型的优化目标和损失函数的定义方式

四十一、为什么引入非线性激活函数？
    1、在线性激活函数下，多层网络和一层网络相当
    2、非线性相当于对空间进行变换，对空间问题进行化简，原来线性不可解变成可解
    3、如果不用激活函数，每一层的输出都是上一层的线性函数，这样无论神经网络是多少层，输出都是输入的线性组合，这样与没有隐藏层效果相当
    
四十二、二叉树的4种遍历方法
    1、前序（中 → 左 → 右）
    2、后序（左 → 右 → 中）
    3、中序（左 → 中 → 右）
    4、层序（逐层访问，从左到右）
    可以看出，是“中”的位置决定了命名
    https://blog.csdn.net/weixin_44523387/article/details/91313293
    https://www.cnblogs.com/fly-me/p/wei-ti-jiaoer-cha-shu-de-si-zhong-bian-li-fang-fa.html
    
    """
    层序遍历的Python实现（迭代版）
    Desc: 自下而上 从左往右
    Note: 迭代版
    """
    class BinaryTree(object):
        def __init__(self, parent=None, left=None, right=None):
            self.parent = parent
            self.left = left
            self.right = right
        
    class LevelWise(object):
        def __init__(self, root=None):
            self.root = root
        
        def level_order(self):
            """
            Return:
                result:list
            """
            if not self.root:
                return []
            stack = [self.root]
            result = []
            
            while len(stack) > 0:
                level = []
                item = []
                for i in stack:
                    item.append(i.parent)
                    if i.left:  # 从左往右的实现
                        level.append(i.left)
                    if i.right:  # 注意：这里elif和if的区别，重大失误啊
                        level.append(i.right)
                stack = level
                result.insert(0, item)  #  这里实现层序遍历的自下而上
            return result

    if __name == '__mian__':
        node_1 = BinaryTree(parent=4)
        node_2 = BinaryTree(parent=5)
        node_3 = BinaryTree(parent=2)
        node_4 = BinaryTree(parent=3, left=node_1, right=node_2)
        root = BinaryTree(parent=1, left=node_3, right=node_4)
        lw = LevelWise(root)
        print("Level-wise enumerate for Binary tree:{}".format(lw.level_order()))

https://www.cnblogs.com/liutongqing/p/7698429.html

四十三、神经网络中，激活函数ReLU为什么要好于sigmoid和tanh？
    1、采用sigmoid/tanh在反向传播计算误差梯度时，求导除法，涉及指数运算，计算量相当大
    2、对于深层网络，sigmoid在反向传播时，很容易出现梯度消失的情形（在接近饱和区域，导数区域0）
    3、Relu会使一部分神经元的输出为0，这样就造成了网络的稀疏性，并且减少了参数的相互依存关系，缓解了过拟合问题的发生

四十四、树
    树 二叉树 二叉查找树 平衡二叉查找树 红黑树 递归树
    树是一种非常高校的非线性存储结构（树 根节点 叶子节点 同一层的叶子节点叫做兄弟节点 邻近但不同层的叫做父子节点）
    二叉树：每一个节点上至多有2个叶子节点的树
    满二叉树：除叶子节点外，每个节点都有左右两个子节点
    完全二叉树：若设二叉树的深度为k，除第 k 层外，其它各层 (1～k-1) 的结点数都达到最大个数，第k层所有的结点都连续集中在最左边
    平衡二叉树(AVL):它或者是一颗空树，或者具有以下性质的二叉排序树：它的左子树和右子树的深度之差(平衡因子)的绝对值不超过1，且它的左子树和右子树都是一颗平衡二叉树
        一棵AVL树有如下必要条件：
        条件1：它必须是二叉查找树
        条件2：每个节点的左子树和右子树的高度差至多为1
    
    二叉树的删除节点（按照二叉查找树的方式删除）
    二叉查找树，又称二叉搜索树(Binary Search Tree)
    它是特殊的二叉树：对于二叉树，假设x为二叉树中的任意一个结点，x节点包含关键字key，节点x的key值记为key[x]。
    如果y是x的左子树中的一个结点，则key[y] <= key[x]；
    如果y是x的右子树中的一个结点，则key[y] >= key[x]。
    
    在二叉查找树中：
    (1) 若任意节点的左子树不空，则左子树上所有结点的值均小于它的根结点的值；
    (2) 任意节点的右子树不空，则右子树上所有结点的值均大于它的根结点的值；
    (3) 任意节点的左、右子树也分别为二叉查找树；
    (4) 没有键值相等的节点（no duplicate nodes）。
    
    节点的前驱(predecessor)：是该节点的左子树中的最大节点。
    节点的后继(successor)：是该节点的右子树中的最小节点。
    
    
    二叉树的节点删除分3种情况：
    1、没有左右子节点，即被删除的节点是叶子节点，则该节点可以直接删除
    2、仅仅存在左节点或者右节点，删除后需要对子节点移动
    3、同时存在左右子节点，不能简单的删除，但是可以通过和后继节点（严格来说是中序后继节点）交换后转换为前两种情况
        该找哪个节点来替换待删节点呢？
        结论是，需要在树中找出全部比待删节点的值大的全部数。并在这些数中找出一个最小的数来。
        听起来非常拗，假设把它用图形来描写叙述的话，就是，从被删除的节点出发经过它的右节点。
        然后右节点最左边的叶子节点就是我们要找的，它有一个专业名词叫中序后继节点。
    
    BFS(Breadth First Search)和DFS(Deep First Search)
    
    
