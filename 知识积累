一、快速排序
def partition(alist, first, last):
    n = len(alist)
    pivot_value = alist[first]
    left = first + 1
    right = last
    while left <= right:
        if alist[left] > pivot_value and alist[right] < pivot_table:
            alist[left], alist[right] = alist[right], alist[left]
            left += 1
            right -= 1
        elif alist[left] < pivot_value:
            left += 1
        elif alist[right] > pivot_value:
            right -= 1
    alist[first], alist[right] = alist[right], alist[first]
    return right
  
def quick_sort_helper(alist, first, last):
    if first < last:
        split_point = partition(alist, first, last)
        quick_sort_helper(alist, first, split_point-1)
        quick_sort_helper(alist, split_point+1, right)

def quick_sort(alist):
    first = 0
    last = len(alist) - 1
    quick_sort_helper(alist, first, last)
    return alist

二、二分查找
1、递归版
def bisec_find(alist, item):
    if len(alist) == 0:
        return False
    else: 
        mid = len(alist) // 2
        if len(alist) > 1:
            if alist[mid] == item:
                return True
            elif alist[mid] > item:
                return bisec_find(alist[:mid], item)
            else:
                return bisec_find(alist[mid+1:], item)
        
2、迭代版
def bisec_find(alist, item):
    n = len(alist)
    left = 0
    right = n - 1
    while left < right:
        mid = (left + right) // 2
        if alist[mid] == item:
            return True
            break
        elif alist[mid] > item:
            right = mid - 1
        else:
            left = mid + 1
    return False


三、简要介绍一下SVM

四、SVM中常用的核函数
核函数将原来的输入空间映射到一个新的特征空间，在新的特征空间里从训练样本中学习线性支持向量机。
当特征函数是非线性时，学习到的含有核函数的支持向量机就是非线性分类模型
    1、线性核
    2、多项式核
    3、高斯核（径向基）
    

五、SVM和LR的异同

六、过拟合(overfitting)怎么解决？
其直观的表现如下：随着训练过程的进行，模型复杂度的增加，在训练集上的误差渐渐减小，但是在验证集上的误差反而增大
泛化(generalization) 训练集 测试集
原因：噪声数据
    1、增大样本量（如果可以）
        Data Augmentation 数据扩增，图像识别中的平移、旋转和缩放
    2、Regularization(正则化)
    3、Cross Validation(交叉验证)
    4、Dropout(随机失活)
    在神经网络的训练过程中，神经元以超参数p的概率被激活(即1-p的概率被屏蔽)，其效果类似于集成学习中Bagging
    5、Batch Normalization(逐层归一化)
    每一层的输出都做归一化
    6、Early Stopping(提前终止)

七、Logistic Regression的原理

八、GBDT和XGBoost的区别
XGBoost本质上还是GBDT，其优化主要体现在工程设计层面，包括并发的程序执行，贪心的排序操作等
    1、GBDT以CART为基分类器，XGBoost还支持线性分类器
    2、GBDT在优化时只用到一阶导数的信息（负梯度），而XGBoost则对损失函数进行二阶泰勒展开，同时用到一阶和二阶导数信息；且XGBoost支持自定义损失函数，
只要函数是一阶和二阶可导
    3、XGBoost在损失函数中加入了正则项，控制模型的复杂度，有效防止过拟合，提高模型的泛化能力
    正则项中包括（1）树的叶子节点个数（2）每个叶子节点上输出的score的L2模的平方和
    4、XGBoost支持并行
    XGBoost的并行不是tree粒度的并行，作为Boosting方法，同样是一次迭代完再进行下一次的迭代。XBGoost的并行体现在特征粒度上：
    （1）近似算法（Weighted Quantile Sketch）
    树的分裂过程最耗时的一个步骤就是对特征的值进行排序，采用贪心算法枚举所有可能的分割节点，确定最佳分割节点
    XGBosot在训练之前，预先对数据进行了排序，然后保存为block结构，后面的迭代重复使用这个结构，大大减少计算量，这个block结构也使得并行成为可能。
    近似算法（分桶后再greedy搜索）中很重要的一步就是提出备选分割点，通常情况下，均匀挑选一个特征的百分位数作为候选。
    对于每个分割点，其函数值是用此分位点下包含的二阶导之和的比例。
    即相邻的两个分割点之间的间隔小于给定的ε。这里的ε是一个近似系数。直观上看，这大致表示，总共选出1/ε个候选点。这里每一个点都按照其二阶导h进行加权。
    （2）近似直方图算法（Distributed Weighted Quantile Sketch）
    贪心算法在数据无法一次载入内存或者在分布式情况下变得低效，可并行的近似直方图算法，高效地生成候选分割点
    5、shrinkage（缩减）
    相当于学习速率。XGBoost在进行完一次迭代后，会将叶子节点的权重乘上该系数，主要是为了削弱每棵树的影响，为后面的树去提供空间优化模型
    6、column subsampling
    列(特征)抽样，随机森林那边学习来的，防止过拟合的效果比传统的行抽样还好（行抽样功能也有），并且有利于后面提到的并行化处理算法
    7、缺失值的处理
    当特征出现缺失值时，XGBoost可以学习出默认的节点分裂方向。
    Xgboost的处理方式是，缺失值数据会被分到左子树和右子树分别计算损失，选择较优的那个（Max Gain）。
    如果训练中没有数据缺失，预测时出现了数据缺失，默认分类到右子树。

九、为什么XGBoost要使用二阶泰勒展开？优势在哪里？
    1、有利于梯度下降得更快更准
    2、可以在不选定损失函数具体形式的情况下，仅仅依靠输入数据的值就可以进行叶子分裂计算。本质上是把损失函数的选取和参数优化分开。这种去耦合增加了XGBoost
    的适用性，使得它可以按需选择损失函数。
    
十、LR和线性回归的区别和联系
    1、目的
        分类 回归
    2、损失函数
        对数似然损失函数 平方和损失函数

十一、生成式模型和判别式模型
    生成式：由数据直接学习决策函数Y=f(x)或者条件分布概率P(Y|x)作为预测模型
    判别式：先由数据学习联合概率密度P(x,y)，再求出条件概率密度P(Y|x)
    
十二、CNN最成功的应用是在CV，那为什么NLP和Speech的很多问题也可以用？这几个不相关问题的相似性在哪里？CNN通过什么手段抓住了这些共性？
    都存在整体和局部的关系 Global Local
    由低层次的特征组合成高层次的特征 Low-level High-level
    CNN抓住共性的手段主要有4个：
    1、局部连接（也称稀疏交互Sparse Interaction）
    提取数据的局部特征
    2、权值共享（也称参数共享Parameter Sharing）
    权值共享大大降低了网络的训练难度，一个卷积核（Filter）只提取一个特征
    3、池化操作
    池化操作与多层次结构一起，实现了数据的降维，将低层次的局部特征组合成较高层次的特征，从而实现对整体图片的表示
    4、多层次结构

十三、LSTM结构推导？为什么比RNN好？LSTM如何解决梯度消失的问题？
    CNN → RNN(梯度消失) → LSTM
    RNN反向传播更新参数采用的链式法则，会出现梯度爆炸的问题
    
十四、拼写检查/拼写纠正
    基于贝叶斯方法

十五、对比pLSA和LDA的区别

十六、为什么要归一化？
    1、提升梯度下降法求解最优解的速度（少走弯路）
    2、距离计算中，消除量纲的影响（KNN / SVM）
    一般算法如果本身受量纲影响较大（KNN / SVM），或者损失函数（最优化方法采用梯度下降）受量纲影响较大（线性回归 / 逻辑回归），则需要对特征进行归一化。
    LR本身不收量纲的影响，但是采用梯度下降法优化参数时如果不进行归一化会导致参数迭代缓慢，影响效率。
    
十七、LR为什么要对输入的连续特征进行离散化？
    1、离散化后的特征对异常值具有很好的鲁棒性
    2、单个连续性变量离散化为N个分类型变量后，每个变量都有其单独的权重，相当于为为模型引入了非线性性，提升模型的表达能力，减少欠拟合
    3、离散化后可以进行特征交叉，由M+N个变量变成M×N个变量，进一步引入非线性性，提升模型表达能力
    李沐曾经说过：模型是使用离散特征还是连续特征，其实是一个“海量离散特征+简单模型” 同 “少量连续特征+复杂模型”的权衡。
    既可以离散化用线性模型，也可以用连续特征加深度学习。就看是喜欢折腾特征还是折腾模型了。
    通常来说，前者容易，而且可以n个人一起并行做，有成功经验；后者目前看很赞，能走多远还须拭目以待。

https://blog.csdn.net/jifeihuiaoguai988/article/details/51378108  EM算法介绍

十八、L1和L2正则的先验分别服从什么分布？
    L1 拉普拉斯分布  在均值附近，比正态分布更为陡峭（位置参数mu，尺度参数sigma）
    L2 正态分布
    
十九、完整的机器学习项目的流程
    误差分析/模型诊断
    误差分析：通过观察误差样本，全面分析误差产生的原因，是特征的问题还是数据本身的问题？是算法选择的问题还是参数的问题？
    诊断后的模型需要重新进行调优，调优后的模型需要重新进行诊断，不断循环往复，达到预期效果
    
    上线运行
    运行速度（时间复杂度）
    资源消耗（空间复杂度）
    稳定性
    
二十、梯度消失和梯度爆炸
    2种情况，梯度消失经常出现
    1、梯度消失（Gradient Vanish）
        1）深层网络中
        基于反向传播进行权值更新的过程中，根据链式法则进行连乘，如果激活函数求导值<1，随着层数的增加，求出的权重更新将以指数级衰减，即发生梯度消失
        2）采用不合适的损失函数
        比如Sigmoid，tanh比sigmoid要好一些，但是它的导数仍然是小于1
    2、 梯度爆炸（Gradient Explosion）
        1）深层网络中
        2）权值初始化值太大
    3、如何解决？
        1）梯度剪切
        设置一个梯度剪切阈值，将其强行限定在某个区间范围
        2）权重正则化
        避免过大的权值出现，防止梯度爆炸
        3）激活函数
        ReLU        
            优：解决了梯度消失、爆炸的问题；计算方便，速度快，加速了网络的训练速度
            缺：负数部分恒为0，导致神经元无法激活
        4）Batch Normalization（批量归一化）
        加速网络训练速度，提升训练稳定性
        5）残差结构
        残差的跨层连接结构，即残差的捷径（shortcut）部分
        6）LSTM
        门控机制
https://blog.csdn.net/qq_25737169/article/details/78847691
https://blog.csdn.net/qq_25737169/article/details/79048516

二十一、什么是卷积？
    对图像的局部数据窗口和滤波矩阵做内积（对应位置元素相乘再相加）的操作即为卷积。

二十二、什么是池化？
    取区域内的平均或者最大
    局部连接
    
二十三、简述下什么是生成对抗网络？
    生成对抗网络（Generative Adversarial Network），其内部是竞争关系。有点类似“以子之矛，攻子之盾”在相爱相杀中共同成长。
    一方是生成模型（Generative Model），另一方是判别模型（Discriminative Model）
    生成模型的任务是生成一个实例来骗过判别模型，而判别模型的任务则是判断这个实例是来自训练样本（真实）还是生成模型（合成）。
    双方相互对抗，最终达到一个平衡。

二十四、HMM和CRF的区别
    1、模式
    HMM：生成式
    CRF：判别式
    2、是否先验
    HMM：加入对状态概率的先验知识
    CRF：完全是Data Driven

二十五、k-d树
    k-d树（k-dimension tree的缩写）一种空间划分树，其本质是一种平衡二叉树，主要应用于多维空间关键数据的搜索（空间搜索和最近领搜索）
    k-d树的构建是一个递归的过程。采用分而治之的思想，将整个空间划分几个小的空间。
    确定Split域：对于所有描述子数据（特征矢量），统计他在每个维度的方差，选取方差中的最大值，对应的就是split域。方差最大表示沿着这个
    坐标轴方向数据分散的比较开，这个方向进行分割能获得比较好的分辨率。
    k-d树构建后，如何进行最近领搜索 https://img-my.csdn.net/uploads/201206/03/1338711447_6884.gif
    
    https://blog.csdn.net/v_july_v/article/details/8203674
    
二十六、LSTM如何解决梯度消失的问题？
    https://www.cnblogs.com/xumaomao/articles/11071744.html
    https://www.cnblogs.com/bonelee/p/10475453.html

二十七、PCA的主要步骤？
    
二十八、LDA(Latent Dirichlet Allocation)和PLSA的区别
    在贝叶斯框架下的LDA，我们不再认为主题分布（文档中各个主题的出现概率）和词分布（主题中各个词语出现的概率分布）不是唯一确定的，而是随机变量
    有点类似频率学派和贝叶斯学派的争议

二十九、为什么需要对数据进行归一化？
    （1）有利于加速梯度下降法的收敛速度，对于使用梯度下降进行参数优化的算法
    （2）涉及距离计算、模型中含有公式，量纲的存在会干扰参数估计的结果，最终体现的是特征的重要性
    树形模型（概率模型）不需要归一化

三十、深度（监督）学习在计算机视觉（CV）领域的进展
    1、图像分类（Image Classification）
    ImageNet大规模图像识别竞赛
    各种CNN的变种：
       AlexNet(2013 83.6%) → VGG(2014 92.7%) → GoogleNet(2014 93.3%) → ResNet（2015 96.43% 微软 识别正确率超过人类水平） 
    2、图像/目标检测（Image/Object Detection）
    图像检测是指在分类图像的同时把物体用矩形框圈定起来 Classification + Localization
    先后涌现R-CNN(Region CNN) → Fast R-CNN() → Faster R-CNN() → YOLO → SSD
    https://blog.csdn.net/v_JULY_v/article/details/80170182  一文读懂目标检测：R-CNN、Fast R-CNN、Faster R-CNN、YOLO、SSD
    3、图像分割（Senmatic Segmentation）
    图像分割是把图片中不同的物体用不同的颜色分割出来
    4、图像标注-看图说话（Image Captioning）
    根据一张图片，生成一段文字描述它
    5、图像生成-文字转图片（Image Generator）
    根据一段文字，生成对应的图片

三十一、深度学习中的调参经验
    1、参数初始化
    不进行初始化会减慢收敛速度，影响收敛结果，甚至会造成Nan等一系列问题
    （1）uniform均匀分布初始化
    （2）normal高斯分布初始化
    （3）svd初始化 对RNN有比较好的效果
    2、数据预处理
    zero-center
    3、梯度消失/爆炸
    Gradient Clipping梯度裁剪，限制最大梯度
        附：梯度裁剪的执行步骤
            （1）设定超参数梯度阈值clip_grad的值
            （2）在back propagation的过程中得到各个参数的梯度后，并不直接用来更新参数。而是先计算这些梯度的L2范数L2_grad
            （3）比较L2_grad与clip_grad阈值的大小，如果L2_grad > clip_grad，计算缩放因子scale=clip_grad/L2_grad，否则，scale=1
            （4）g_origin×scale进行缩放，然后进行参数更新
    替代激活函数Sigmoid → tanh → ReLU
    梯度归一化
    4、防止过拟合
    Dropout
    Batch Normalization
    
三十二、RNN、LSTM和GRU的区别
    RNN：处理序列型数据，引入循环的概念，但是在实际过程中却出现初始信息随着时间消失的问题
    LSTM：门控机制，非常适合记忆，解决梯度消失的问题
    GRU：门控循环单元 Gated Recurrent Unit 更新门和重置门 LSTM的一种变体，将遗忘门和输入门合并成更新门

三十三、当机器学习遇到瓶颈时，如何优化？
    https://blog.csdn.net/han_xiaoyang/article/details/52654879 如何提高机器学习和深度学习的性能
    https://blog.csdn.net/han_xiaoyang/article/details/53453145 机器学习性能改善备忘单
    值得细细品读
    
三十四、什么样的资料不适合做深度学习？
    1、数据集太小时
    深度学习相对机器学习没有优势
    2、数据集没有局部相关性
    目前，在深度学习表现比较好的领域主要有图像、语音、文本（自然语言）领域，这些领域的一个共性是局部相关性：
        图像由像素组成物体、语音信号中音位组成单词、文本数据中单词组成句子，这些特征元素的组合一旦被打乱，表示的含义也会随之变化
        对于没有这样的局部相关性的数据集，不适于使用深度学习算法进行处理。

三十五、准备机器学习面试，应该准备哪些理论知识？
    
三十六、RNN的变体 N vs M
    这种结构又叫做Encoder-Decoder模型，也可以称之为Seq2seq模型
    原始的N vs N RNN要求序列等长，然而我们遇到的大部分问题都是序列不等长的。例如机器翻译中，源语言和目标语言的句子往往并没有相同的长度。
    由于这种Encoder-Decoder模型结构不限制输入和输出序列的长度，因此其应用范围十分广泛
    机器翻译 Encoder-Decoder的最经典应用
    文本摘要 输入一段文本序列，输出是这段文本序列的摘要
    阅读理解 输出文章和问题编码，再对其进行解码得到问题的答案
    语音识别 输入语音信号，输出文本序列
    图像标注 看图说话
    
三十七、如何解决神经网络中的梯度爆炸问题？
    1、重新设计网络结构（优化）
    层数更少/批尺寸更小
    RNN → LSTM
    2、选取新的激活函数
    sigmoid/tanh → Relu
    3、梯度截断（Gradient Clipping）
    4、权重正则化（Weight Regularization）
    
三十八、深度学习（CNN/RNN/Attention）解决大规模文本分类问题
    1、传统文本分类方法
    人工特征工程 + 浅层分类模型
    （1）文本预处理：分词（研究表明，特征粒度为词远好于为字，基于字粒度显然损失了过多的n-gram信息） 
                    去停用词 维护一个停用词表
    （2）文本表示和特征处理
    传统方法：词袋（BOW Bag of Word）或者向量空间模型（Vector Space Model）
    缺陷：无法捕捉上下文关系，每个词之间彼此独立，且无法表征语义信息
        词袋法：高维稀疏
        向量空间模型：特征提取部分对应特征选择和特征权重计算（TF-IDF）两个部分
    （3）基于语义的文本表示
    LDA和pLSA文档主题模型
    
    2、现代分词
    Word Embedding + Bi-LSTM + CRF逐渐成为主流
    
    3、深度学习方法
    FastText
    TextCNN（捕捉局部相关性，具体到文本分类任务中就是利用CNN来提取句子中的n-gram关键信息）
    TextRNN
    TextRNN + Attention
    
三十九、对偶问题
    primal/dual 
    给出主问题最优解的下界

四十、推荐系统技术演进趋势
    经典的两阶段：多路召回(Retrieve/Match) + 统一排序(Rank)
    召回强调快：少量特征 + 简单模型
    排序强调准：丰富特征 + 复杂模型
    重排层：去已读、去重、打散、多样性保证、固定类型物品插入（往往是技术和业务策略，提升用户体验）
    1、召回层
    偏向策略型导向，往往想到一个策略，增加一路新的召回。
    召回环节的有监督模型化和一切Embedding化，这是两个相辅相成的发展趋势。
    根据召回路是否有个性化因素区分：
    （1）无个性化
    热门文章 点击率排序
    （2）个性化
    用户兴趣标签
    理论上，任何监督模型都可以用来做召回模型（FM/FFM/DNN）
    常说的所谓“双塔”，其实就是用户侧和物品侧特征分离，分别打上Embedding，并非具体的模型
    如果在召回阶段使用模型，那么应该同步采用和排序阶段相同的优化目标，尤其是排序阶段如果采用多目标优化，那么召回阶段也应当采用多目标优化
    
    知识图谱融合召回
    知识图谱是一种信息拓展模式，对知识进行近距离的拓展，可以起到信息补充作用
    比较适合用户行为非常稀疏以及用户冷启动的场景
    这种
