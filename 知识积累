一、快速排序
def partition(alist, first, last):
    n = len(alist)
    pivot_value = alist[first]
    left = first + 1
    right = last
    while left <= right:
        if alist[left] > pivot_value and alist[right] < pivot_table:
            alist[left], alist[right] = alist[right], alist[left]
            left += 1
            right -= 1
      elif alist[left] < pivot_value:
          left += 1
      elif alist[right] > pivot_value:
          right -= 1
    alist[first], alist[right] = alist[right], alist[first]
    return right
  
def quick_sort_helper(alist, first, last):
    if first < last:
        split_point = partition(alist, first, last)
        quick_sort_heiper(alist, first, split_point-1)
        quick_sort_heiper(alist, split_point+1, right)

def quick_sort(alist):
    first = 0
    last = len(alist) - 1
    quick_sort_helper(alist, first, last)
    return alist

二、二分查找
1、递归版
def bisec_find(alist, item):
    if len(alist) == 0:
        return False
    else: 
        mid = len(alist) // 2
        if len(alist) > 1:
            if alist[mid] == item:
                return True
            elif alist[mid] > item:
                return bisec_find(alist[:mid], item)
            else:
                return bisec_find(alist[mid+1:], item)
        
2、迭代版
def bisec_find(alist, item):
    n = len(alist)
    left = 0
    right = n - 1
    while left < right:
        mid = (left + right) // 2
        if alist[mid] == item:
            return True
            break
        elif alist[mid] > item:
            right = mid - 1
        else:
            left = mid + 1
    return False


三、简要介绍一下SVM

四、SVM中常用的核函数
核函数将原来的输入空间映射到一个新的特征空间，在新的特征空间里从训练样本中学习线性支持向量机。
当特征函数是非线性时，学习到的含有核函数的支持向量机就是非线性分类模型
    1、线性核
    2、多项式核
    3、高斯核（径向基）
    

五、SVM和LR的异同

六、过拟合(overfitting)怎么解决？
其直观的表现如下：随着训练过程的进行，模型复杂度的增加，在训练集上的误差渐渐减小，但是在验证集上的误差反而增大
泛化(generalization) 训练集 测试集
原因：噪声数据
    1、增大样本量（如果可以）
        Data Augmentation 数据扩增，图像识别中的平移、旋转和缩放
    2、Regularization(正则化)
    3、Cross Validation(交叉验证)
    4、Dropout(随机失活)
    在神经网络的训练过程中，神经元以超参数p的概率被激活(即1-p的概率被屏蔽)，其效果类似于集成学习中Bagging
    5、Batch Normalization(逐层归一化)
    每一层的输出都做归一化
    6、Early Stopping(提前终止)

七、Logistic Regression的原理

八、GBDT和XGBoost的区别
XGBoost本质上还是GBDT，其优化主要体现在工程设计层面，包括并发的程序执行，贪心的排序操作等
    1、GBDT以CART为基分类器，XGBoost还支持线性分类器
    2、GBDT在优化时只用到一阶导数的信息（负梯度），而XGBoost则对损失函数进行二阶泰勒展开，同时用到一阶和二阶导数信息；且XGBoost支持自定义损失函数，
只要函数是一阶和二阶可导
    3、XGBoost在损失函数中加入了正则项，控制模型的复杂度，有效防止过拟合，提高模型的泛化能力
    正则项中包括（1）树的叶子节点个数（2）每个叶子节点上输出的score的L2模的平方和
    4、XGBoost支持并行
    XGBoost的并行不是tree粒度的并行，作为Boosting方法，同样是一次迭代完再进行下一次的迭代。XBGoost的并行体现在特征粒度上：
    （1）近似算法（Weighted Quantile Sketch）
    树的分裂过程最耗时的一个步骤就是对特征的值进行排序，采用贪心算法枚举所有可能的分割节点，确定最佳分割节点
    XGBosot在训练之前，预先对数据进行了排序，然后保存为block结构，后面的迭代重复使用这个结构，大大减少计算量，这个block结构也使得并行成为可能。
    近似算法（分桶后再greedy搜索）中很重要的一步就是提出备选分割点，通常情况下，均匀挑选一个特征的百分位数作为候选。
    对于每个分割点，其函数值是用此分位点下包含的二阶导之和的比例。
    即相邻的两个分割点之间的间隔小于给定的ε。这里的ε是一个近似系数。直观上看，这大致表示，总共选出1/ε个候选点。这里每一个点都按照其二阶导h进行加权。
    （2）近似直方图算法（Distributed Weighted Quantile Sketch）
    贪心算法在数据无法一次载入内存或者在分布式情况下变得低效，可并行的近似直方图算法，高效地生成候选分割点
    5、shrinkage（缩减）
    相当于学习速率。XGBoost在进行完一次迭代后，会将叶子节点的权重乘上该系数，主要是为了削弱每棵树的影响，为后面的树去提供空间优化模型
    6、column subsampling
    列(特征)抽样，随机森林那边学习来的，防止过拟合的效果比传统的行抽样还好（行抽样功能也有），并且有利于后面提到的并行化处理算法
    7、缺失值的处理
    当特征出现缺失值时，XGBoost可以学习出默认的节点分裂方向。
    Xgboost的处理方式是，缺失值数据会被分到左子树和右子树分别计算损失，选择较优的那个（Max Gain）。
    如果训练中没有数据缺失，预测时出现了数据缺失，默认分类到右子树。

九、为什么XGBoost要使用二阶泰勒展开？优势在哪里？
    1、有利于梯度下降得更快更准
    2、可以在不选定损失函数具体形式的情况下，仅仅依靠输入数据的值就可以进行叶子分裂计算。本质上是把损失函数的选取和参数优化分开。这种去耦合增加了XGBoost
    的适用性，使得它可以按需选择损失函数。
    
十、LR和线性回归的区别和联系
    1、目的
        分类 回归
    2、损失函数
        对数似然损失函数 平方和损失函数

十一、生成式模型和判别式模型
    生成式：由数据直接学习决策函数Y=f(x)或者条件分布概率P(Y|x)作为预测模型
    判别式：先由数据学习联合概率密度P(x,y)，再求出条件概率密度P(Y|x)
    
十二、CNN最成功的应用是在CV，那为什么NLP和Speech的很多问题也可以用？这几个不相关问题的相似性在哪里？CNN通过什么手段抓住了这些共性？
    都存在整体和局部的关系 Global Local
    由低层次的特征组合成高层次的特征 Low-level High-level
    CNN抓住共性的手段主要有4个：
    1、局部连接（也称稀疏交互Sparse Interaction）
    提取数据的局部特征
    2、权值共享（也称参数共享Parameter Sharing）
    权值共享大大降低了网络的训练难度，一个卷积核（Filter）只提取一个特征
    3、池化操作
    池化操作与多层次结构一起，实现了数据的降维，将低层次的局部特征组合成较高层次的特征，从而实现对整体图片的表示
    4、多层次结构

十三、LSTM结构推导？为什么比RNN好？LSTM如何解决梯度消失的问题？
    CNN → RNN(梯度消失) → LSTM
    RNN反向传播更新参数采用的链式法则，会出现梯度爆炸的问题
    
十四、拼写检查/拼写纠正
    基于贝叶斯方法

十五、对比pLSA和LDA的区别

十六、为什么要归一化？
    1、提升梯度下降法求解最优解的速度（少走弯路）
    2、距离计算中，消除量纲对重要性权重的影响
    
十七、LR为什么要对输入的连续特征进行离散化？
    1、离散化后的特征对异常值具有很好的鲁棒性
    2、单个连续性变量离散化为N个分类型变量后，每个变量都有其单独的权重，相当于为为模型引入了非线性性，提升模型的表达能力，减少欠拟合
    3、离散化后可以进行特征交叉，由M+N个变量变成M×N个变量，进一步引入非线性性，提升模型表达能力
    李沐曾经说过：模型是使用离散特征还是连续特征，其实是一个“海量离散特征+简单模型” 同 “少量连续特征+复杂模型”的权衡。
    既可以离散化用线性模型，也可以用连续特征加深度学习。就看是喜欢折腾特征还是折腾模型了。
    通常来说，前者容易，而且可以n个人一起并行做，有成功经验；后者目前看很赞，能走多远还须拭目以待。

https://blog.csdn.net/jifeihuiaoguai988/article/details/51378108  EM算法介绍

十八、L1和L2正则的先验分别服从什么分布？
    L1 拉普拉斯分布  在均值附近，比正态分布更为陡峭（位置参数mu，尺度参数sigma）
    L2 正态分布
    
十九、完整的机器学习项目的流程
    误差分析/模型诊断
    
二十、梯度消失和梯度爆炸
    2种情况，梯度消失经常出现
    1、梯度消失（Gradient Vanish）
        1）深层网络中
        基于反向传播进行权值更新的过程中，根据链式法则进行连乘，如果激活函数求导值<1，随着层数的增加，求出的权重更新将以指数级衰减，即发生梯度消失
        2）采用不合适的损失函数
        比如Sigmoid，tanh比sigmoid要好一些，但是它的导数仍然是小于1
    2、 梯度爆炸（Gradient Explosion）
        1）深层网络中
        2）权值初始化值太大
    3、如何解决？
        1）梯度剪切
        设置一个梯度剪切阈值，将其强行限定在某个区间范围
        2）权重正则化
        避免过大的权值出现，防止梯度爆炸
        3）激活函数
        ReLU        
            优：解决了梯度消失、爆炸的问题；计算方便，速度快，加速了网络的训练速度
            缺：负数部分恒为0，导致神经元无法激活
        4）Batch Normalization（批量归一化）
        加速网络训练速度，提升训练稳定性
        5）残差结构
        残差的跨层连接结构，即残差的捷径（shortcut）部分
        6）LSTM
        门控机制
https://blog.csdn.net/qq_25737169/article/details/78847691
https://blog.csdn.net/qq_25737169/article/details/79048516

二十一、什么是卷积？
    对图像的局部数据窗口和滤波矩阵做内积（对应位置元素相乘再相加）的操作即为卷积。

二十二、什么是池化？
    取区域内的平均或者最大
    局部连接
    
二十三、简述下什么是生成对抗网络？
    生成对抗网络（Generative Adversarial Network），其内部是竞争关系。有点类似“以子之矛，攻子之盾”在相爱相杀中共同成长。
    一方是生成模型（Generative Model），另一方是判别模型（Discriminative Model）
    生成模型的任务是生成一个实例来骗过判别模型，而判别模型的任务则是判断这个实例是来自训练样本（真实）还是生成模型（合成）。
    双方相互对抗，最终达到一个平衡。

二十四、HMM和CRF的区别
    1、模式
    HMM：生成式
    CRF：判别式
    2、是否先验
    HMM：加入对状态概率的先验知识
    CRF：完全是Data Driven

二十五、k-d树
    k-d树（k-dimension tree的缩写）一种空间划分树，其本质是一种平衡二叉树，主要应用于多维空间关键数据的搜索（空间搜索和最近领搜索）
    k-d树的构建是一个递归的过程。采用分而治之的思想，将整个空间划分几个小的空间。
    确定Split域：对于所有描述子数据（特征矢量），统计他在每个维度的方差，选取方差中的最大值，对应的就是split域。方差最大表示沿着这个
    坐标轴方向数据分散的比较开，这个方向进行分割能获得比较好的分辨率。
    k-d树构建后，如何进行最近领搜索 https://img-my.csdn.net/uploads/201206/03/1338711447_6884.gif
    
    https://blog.csdn.net/v_july_v/article/details/8203674
    
二十六、LSTM如何解决梯度消失的问题？
    https://www.cnblogs.com/xumaomao/articles/11071744.html
    https://www.cnblogs.com/bonelee/p/10475453.html

二十七、PCA的主要步骤？
    
二十八、LDA(Latent Dirichlet Allocation)和PLSA的区别
    在贝叶斯框架下的LDA，我们不再认为主题分布（文档中各个主题的出现概率）和词分布（主题中各个词语出现的概率分布）不是唯一确定的，而是随机变量
    有点类似频率学派和贝叶斯学派的争议

二十九、为什么需要对数据进行归一化？
    （1）有利于加速梯度下降法的收敛速度，对于使用梯度下降进行参数优化的算法
    （2）涉及距离计算、模型中含有公式，量纲的存在会干扰参数估计的结果，最终体现的是特征的重要性
    树形模型（概率模型）不需要归一化
