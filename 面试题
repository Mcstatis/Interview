一、快速排序
def partition(alist, first, last):
    n = len(alist)
    pivot_value = alist[first]
    left = first + 1
    right = last
    while left <= right:
        if alist[left] > pivot_value and alist[right] < pivot_table:
            alist[left], alist[right] = alist[right], alist[left]
            left += 1
            right -= 1
      elif alist[left] < pivot_value:
          left += 1
      elif alist[right] > pivot_value:
          right -= 1
    alist[first], alist[right] = alist[right], alist[first]
    return right
  
def quick_sort_helper(alist, first, last):
    if first < last:
        split_point = partition(alist, first, last)
        quick_sort_heiper(alist, first, split_point-1)
        quick_sort_heiper(alist, split_point+1, right)

def quick_sort(alist):
    first = 0
    last = len(alist) - 1
    quick_sort_helper(alist, first, last)
    return alist

二、二分查找
1、递归版
def bisec_find(alist, item):
    if len(alist) == 0:
        return False
    else: 
        mid = len(alist) // 2
        if len(alist) > 1:
            if alist[mid] == item:
                return True
            elif alist[mid] > item:
                return bisec_find(alist[:mid], item)
            else:
                return bisec_find(alist[mid+1:], item)
        
2、迭代版
def bisec_find(alist, item):
    n = len(alist)
    left = 0
    right = n - 1
    while left < right:
        mid = (left + right) // 2
        if alist[mid] == item:
            return True
            break
        elif alist[mid] > item:
            right = mid - 1
        else:
            left = mid + 1
    return False


三、简要介绍一下SVM

四、SVM中常用的核函数
核函数将原来的输入空间映射到一个新的特征空间，在新的特征空间里从训练样本中学习线性支持向量机。
当特征函数是非线性时，学习到的含有核函数的支持向量机就是非线性分类模型
    1、线性核
    2、多项式核
    3、高斯核（径向基）
    

五、SVM和LR的异同

六、过拟合(overfitting)怎么解决？
其直观的表现如下：随着训练过程的进行，模型复杂度的增加，在训练集上的误差渐渐减小，但是在验证集上的误差反而增大
泛化(generalization) 训练集 测试集
原因：噪声数据
    1、增大样本量（如果可以）
    2、Regularization(正则化)
    3、Cross Validation(交叉验证)
    4、Dropout(随机失活)
    在神经网络的训练过程中，神经元以超参数p的概率被激活(即1-p的概率被屏蔽)，其效果类似于集成学习中Bagging
    5、Batch Normalization(逐层归一化)
    每一层的输出都做归一化
    6、Early Stopping(提前终止)

七、Logistic Regression的原理

八、GBDT和XGBoost的区别
XGBoost本质上还是GBDT，其优化主要体现在工程设计层面，包括并发的程序执行，贪心的排序操作等
    1、GBDT以CART为基分类器，XGBoost还支持线性分类器
    2、GBDT在优化时只用到一阶导数的信息（负梯度），而XGBoost则对损失函数进行二阶泰勒展开，同时用到一阶和二阶导数信息；且XGBoost支持自定义损失函数，
只要函数是一阶和二阶可导
    3、XGBoost在损失函数中加入了正则项，控制模型的复杂度，有效防止过拟合，提高模型的泛化能力
    正则项中包括（1）树的叶子节点个数（2）每个叶子节点上输出的score的L2模的平方和
    4、XGBoost支持并行
    XGBoost的并行不是tree粒度的并行，作为Boosting方法，同样是一次迭代完再进行下一次的迭代。XBGoost的并行体现在特征粒度上：
    （1）近似算法（Weighted Quantile Sketch）
    树的分裂过程最耗时的一个步骤就是对特征的值进行排序，采用贪心算法枚举所有可能的分割节点，确定最佳分割节点
    XGBosot在训练之前，预先对数据进行了排序，然后保存为block结构，后面的迭代重复使用这个结构，大大减少计算量，这个block结构也使得并行成为可能。
    近似算法（分桶后再greedy搜索）中很重要的一步就是提出备选分割点，通常情况下，均匀挑选一个特征的百分位数作为候选。
    对于每个分割点，其函数值是用此分位点下包含的二阶导之和的比例。
    即相邻的两个分割点之间的间隔小于给定的ε。这里的ε是一个近似系数。直观上看，这大致表示，总共选出1/ε个候选点。这里每一个点都按照其二阶导h进行加权。
    （2）近似直方图算法（Distributed Weighted Quantile Sketch）
    贪心算法在数据无法一次载入内存或者在分布式情况下变得低效，可并行的近似直方图算法，高效地生成候选分割点
    5、shrinkage（缩减）
    相当于学习速率。XGBoost在进行完一次迭代后，会将叶子节点的权重乘上该系数，主要是为了削弱每棵树的影响，为后面的树去提供空间优化模型
    6、column subsampling
    列(特征)抽样，随机森林那边学习来的，防止过拟合的效果比传统的行抽样还好（行抽样功能也有），并且有利于后面提到的并行化处理算法
    7、缺失值的处理
    当特征出现缺失值时，XGBoost可以学习出默认的节点分裂方向。
    Xgboost的处理方式是，缺失值数据会被分到左子树和右子树分别计算损失，选择较优的那个（Max Gain）。
    如果训练中没有数据缺失，预测时出现了数据缺失，默认分类到右子树。

九、为什么XGBoost要使用二阶泰勒展开？优势在哪里？
    1、有利于梯度下降得更快更准
    2、可以在不选定损失函数具体形式的情况下，仅仅依靠输入数据的值就可以进行叶子分裂计算。本质上是把损失函数的选取和参数优化分开。这种去耦合增加了XGBoost
    的适用性，使得它可以按需选择损失函数。
    
十、LR和线性回归的区别和联系
    1、目的
        分类 回归
    2、损失函数
        对数似然损失函数 平方和损失函数

十一、生成式模型和判别式模型
