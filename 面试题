一、快速排序
def partition(alist, first, last):
    n = len(alist)
    pivot_value = alist[first]
    left = first + 1
    right = last
    while left <= right:
        if alist[left] > pivot_value and alist[right] < pivot_table:
            alist[left], alist[right] = alist[right], alist[left]
            left += 1
            right -= 1
      elif alist[left] < pivot_value:
          left += 1
      elif alist[right] > pivot_value:
          right -= 1
    alist[first], alist[right] = alist[right], alist[first]
    return right
  
def quick_sort_helper(alist, first, last):
    if first < last:
        split_point = partition(alist, first, last)
        quick_sort_heiper(alist, first, split_point-1)
        quick_sort_heiper(alist, split_point+1, right)

def quick_sort(alist):
    first = 0
    last = len(alist) - 1
    quick_sort_helper(alist, first, last)
    return alist

二、二分查找
1、递归版
def bisec_find(alist, item):
    if len(alist) == 0:
        return False
    else: 
        mid = len(alist) // 2
        if len(alist) > 1:
            if alist[mid] == item:
                return True
            elif alist[mid] > item:
                return bisec_find(alist[:mid], item)
            else:
                return bisec_find(alist[mid+1:], item)
        
2、迭代版
def bisec_find(alist, item):
    n = len(alist)
    left = 0
    right = n - 1
    while left < right:
        mid = (left + right) // 2
        if alist[mid] == item:
            return True
            break
        elif alist[mid] > item:
            right = mid - 1
        else:
            left = mid + 1
    return False


三、简要介绍一下SVM

四、SVM中常用的核函数
核函数将原来的输入空间映射到一个新的特征空间，在新的特征空间里从训练样本中学习线性支持向量机。
当特征函数是非线性时，学习到的含有核函数的支持向量机就是非线性分类模型
    1、线性核
    2、多项式核
    3、高斯核（径向基）
    

五、SVM和LR的异同

六、过拟合(overfitting)怎么解决？
其直观的表现如下：随着训练过程的进行，模型复杂度的增加，在训练集上的误差渐渐减小，但是在验证集上的误差反而增大
泛化(generalization) 训练集 测试集
原因：噪声数据
    1、增大样本量（如果可以）
    2、Regularization(正则化)
    3、Cross Validation(交叉验证)
    4、Dropout(随机失活)
    在神经网络的训练过程中，神经元以超参数p的概率被激活(即1-p的概率被屏蔽)，其效果类似于集成学习中Bagging
    5、Batch Normalization(逐层归一化)
    每一层的输出都做归一化
    6、Early Stopping(提前终止)

七、Logistic Regression的原理

八、GBDT和XGBoost的区别
XGBoost本质上还是GBDT，其优化主要体现在工程设计层面，包括并发的程序执行，贪心的排序操作等
    1、GBDT以CART为基分类器，XGBoost还支持线性分类器
    2、GBDT在优化时只用到一阶导数的信息（负梯度），而XGBoost则对损失函数进行二阶泰勒展开，同时用到一阶和二阶导数信息；且XGBoost支持自定义损失函数，
只要函数是一阶和二阶可导
    3、XGBoost在损失函数中加入了正则项，控制模型的复杂度，有效防止过拟合，提高模型的泛化能力
    正则项中包括（1）树的叶子节点个数（2）每个叶子节点上输出的score的L2模的平方和
    4、XGBoost支持并行
    XGBoost的并行不是tree粒度的并行，作为Boosting方法，同样是一次迭代完再进行下一次的迭代。XBGoost的并行体现在特征粒度上：
    （1）近似算法（Weighted Quantile Sketch）
    树的分裂过程最耗时的一个步骤就是对特征的值进行排序，采用贪心算法枚举所有可能的分割节点，确定最佳分割节点
    XGBosot在训练之前，预先对数据进行了排序，然后保存为block结构，后面的迭代重复使用这个结构，大大减少计算量，这个block结构也使得并行成为可能。
    近似算法（分桶后再greedy搜索）中很重要的一步就是提出备选分割点，通常情况下，均匀挑选一个特征的百分位数作为候选。
    对于每个分割点，其函数值是用此分位点下包含的二阶导之和的比例。
    即相邻的两个分割点之间的间隔小于给定的ε。这里的ε是一个近似系数。直观上看，这大致表示，总共选出1/ε个候选点。这里每一个点都按照其二阶导h进行加权。
    （2）近似直方图算法（Distributed Weighted Quantile Sketch）
    贪心算法在数据无法一次载入内存或者在分布式情况下变得低效，可并行的近似直方图算法，高效地生成候选分割点
    5、shrinkage（缩减）
    相当于学习速率。XGBoost在进行完一次迭代后，会将叶子节点的权重乘上该系数，主要是为了削弱每棵树的影响，为后面的树去提供空间优化模型
    6、column subsampling
    列(特征)抽样，随机森林那边学习来的，防止过拟合的效果比传统的行抽样还好（行抽样功能也有），并且有利于后面提到的并行化处理算法
    7、缺失值的处理
    当特征出现缺失值时，XGBoost可以学习出默认的节点分裂方向。
    Xgboost的处理方式是，缺失值数据会被分到左子树和右子树分别计算损失，选择较优的那个（Max Gain）。
    如果训练中没有数据缺失，预测时出现了数据缺失，默认分类到右子树。

九、为什么XGBoost要使用二阶泰勒展开？优势在哪里？
    1、有利于梯度下降得更快更准
    2、可以在不选定损失函数具体形式的情况下，仅仅依靠输入数据的值就可以进行叶子分裂计算。本质上是把损失函数的选取和参数优化分开。这种去耦合增加了XGBoost
    的适用性，使得它可以按需选择损失函数。
    
十、LR和线性回归的区别和联系
    1、目的
        分类 回归
    2、损失函数
        对数似然损失函数 平方和损失函数

十一、生成式模型和判别式模型
    生成式：由数据直接学习决策函数Y=f(x)或者条件分布概率P(Y|x)作为预测模型
    判别式：先由数据学习联合概率密度P(x,y)，再求出条件概率密度P(Y|x)
    
十二、CNN最成功的应用是在CV，那为什么NLP和Speech的很多问题也可以用？这几个不相关问题的相似性在哪里？CNN通过什么手段抓住了这些共性？
    都存在整体和局部的关系 Global Local
    由低层次的特征组合成高层次的特征 Low-level High-level
    CNN抓住共性的手段主要有4个：
    1、局部连接（也称稀疏交互Sparse Interaction）
    提取数据的局部特征
    2、权值共享（也称参数共享Parameter Sharing）
    权值共享大大降低了网络的训练难度，一个卷积核（Filter）只提取一个特征
    3、池化操作
    池化操作与多层次结构一起，实现了数据的降维，将低层次的局部特征组合成较高层次的特征，从而实现对整体图片的表示
    4、多层次结构

十三、LSTM结构推导？为什么比RNN好？LSTM如何解决梯度消失的问题？
    CNN → RNN(梯度消失) → LSTM
    RNN反向传播更新参数采用的链式法则，会出现梯度爆炸的问题
    
十四、拼写检查/拼写纠正
    基于贝叶斯方法

十五、对比pLSA和LDA的区别

十六、为什么要归一化？
    1、提升梯度下降法求解最优解的速度（少走弯路）
    2、距离计算中，消除量纲对重要性权重的影响
    
十七、LR为什么要对输入的连续特征进行离散化？
    1、离散化后的特征对异常值具有很好的鲁棒性
    2、单个连续性变量离散化为N个分类型变量后，每个变量都有其单独的权重，相当于为为模型引入了非线性性，提升模型的表达能力，减少欠拟合
    3、离散化后可以进行特征交叉，由M+N个变量变成M×N个变量，进一步引入非线性性，提升模型表达能力
    李沐曾经说过：模型是使用离散特征还是连续特征，其实是一个“海量离散特征+简单模型” 同 “少量连续特征+复杂模型”的权衡。
    既可以离散化用线性模型，也可以用连续特征加深度学习。就看是喜欢折腾特征还是折腾模型了。
    通常来说，前者容易，而且可以n个人一起并行做，有成功经验；后者目前看很赞，能走多远还须拭目以待。

https://blog.csdn.net/jifeihuiaoguai988/article/details/51378108  EM算法介绍

十八、L1和L2正则的先验分别服从什么分布？
    L1 拉普拉斯分布  在均值附近，比正态分布更为陡峭（位置参数mu，尺度参数sigma）
    L2 正态分布
    
十九、完整的机器学习项目的流程
    
